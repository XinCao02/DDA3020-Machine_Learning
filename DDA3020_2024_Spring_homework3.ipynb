{"cells":[{"cell_type":"markdown","metadata":{"id":"DbCV8M0vukk1"},"source":["# DDA 3020 Assignment 3: Fully-Connected Neural Networks and CNN\n","\n","Before we start, please put your name and ID in following format:\n","\n","Firstname LASTNAME, #00000000   //  e.g.) Justin JOHNSON, #12345678\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uAMIlt0zxpNc"},"source":["# Your Answer\n","\n","Xin Cao, #120040062"]},{"cell_type":"markdown","metadata":{"id":"WjgisxL9t5W0"},"source":["# Overview\n","\n","In this exercise, we are going to build a **fully-connected neural network** and a **convolutional neural network** from scratch using **numpy** package.\n","\n","For **Fully-connected Neural Network**, you need to implement both forward and backward part to train your model and update the parameters.\n","\n","For **Convolutional Neural Network**, you only need to implement the forward part, including `convolutional layers`, `max_pooling` and `fully-connected layers`. Functions of implementing backpropagation are given. Please check the input requirements for those functions, especially pay attention to the dimension.\n","\n","Please follow the guidance and finish the **<font color=\"green\">[To Do]</font>** and **<font color=\"red\">[Task]</font>** part. Please feel free to define your own functions or modify the functions given if needed. No writing report is required and you will get the mark if the whole process and results are reasonable.\n","\n","If you meet any package or memory issues that can't be solved, feel free to use this colab script to finish your task (remember to copy your own one instead of modifying this colab script directly) : https://colab.research.google.com/drive/1AdGyiVYosU2uMBQ9ahx5B7ZOmH8zEQg_?usp=drive_link"]},{"cell_type":"markdown","metadata":{"id":"rZvuOyO6xv0B"},"source":["# Set Up Code"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3W80z-YfOVyw"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"5si-RE7LQOTV"},"source":["# Fully-connected Neural Network"]},{"cell_type":"markdown","metadata":{"id":"fgdmbFCBI0RL"},"source":["# 1.1 Loading Datasets"]},{"cell_type":"markdown","metadata":{"id":"wasX27MVGMzE"},"source":["In this assignment, we mainly adopt the FashionMnist dataset to train our network. You will be given two csv files: **fashion-mnist_train.csv; fashion-mnist_test.csv**, containing 60000 and 10000 samples each. Each sample is a 28 * 28 grayscale image. In the csv file, each row represents a sample with dimension 1 * 784 (28*28). The first step of this assignment is loading our data. Please run the below cells and a visualization code is also given."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17045,"status":"ok","timestamp":1711826794887,"user":{"displayName":"物午","userId":"12077501645191558009"},"user_tz":-480},"id":"W1ZeWHG1ycc1","outputId":"73ad1445-d94a-49da-8d17-9455d513ba44"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# run it only when you use colab to do this assignment\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# no need to run it if you do this assignment locally\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      4\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"]}],"source":["# run it only when you use colab to do this assignment\n","# no need to run it if you do this assignment locally\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"qw023ZBHQMQx"},"outputs":[],"source":["mnist_train = pd.read_csv('fashion-mnist_train.csv') # change the path to your own path\n","mnist_test = pd.read_csv('fashion-mnist_test.csv') # change the path to your own path"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":492,"status":"ok","timestamp":1711826840596,"user":{"displayName":"物午","userId":"12077501645191558009"},"user_tz":-480},"id":"nH6XUKXXZDjd","outputId":"75a03b9b-6830-49ae-f9b0-14bcb55661cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["(60000, 785)\n"]}],"source":["# check the shape of of the training data\n","print(mnist_train.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":392},"executionInfo":{"elapsed":464,"status":"ok","timestamp":1711826845179,"user":{"displayName":"物午","userId":"12077501645191558009"},"user_tz":-480},"id":"RKUB3KTuZA_i","outputId":"41fd5417-6467-4abc-86b4-f17529125788"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>pixel10</th>\n","      <th>...</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","      <th>pixel784</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.019608</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.117647</td>\n","      <td>0.168627</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.003922</td>\n","      <td>0.007843</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.011765</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.003922</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.019608</td>\n","      <td>0.015686</td>\n","      <td>0.019608</td>\n","      <td>0.019608</td>\n","      <td>0.011765</td>\n","      <td>0.019608</td>\n","      <td>0.023529</td>\n","      <td>...</td>\n","      <td>0.027451</td>\n","      <td>0.031373</td>\n","      <td>0.027451</td>\n","      <td>0.015686</td>\n","      <td>0.011765</td>\n","      <td>0.027451</td>\n","      <td>0.019608</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.054902</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.011765</td>\n","      <td>0.007843</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.003922</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.796078</td>\n","      <td>0.839216</td>\n","      <td>0.650980</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10 rows × 784 columns</p>\n","</div>"],"text/plain":["   pixel1  pixel2  pixel3    pixel4    pixel5    pixel6    pixel7    pixel8  \\\n","0     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n","1     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n","2     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.019608   \n","3     0.0     0.0     0.0  0.003922  0.007843  0.000000  0.000000  0.000000   \n","4     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n","5     0.0     0.0     0.0  0.019608  0.015686  0.019608  0.019608  0.011765   \n","6     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n","7     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n","8     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.011765  0.007843   \n","9     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n","\n","     pixel9   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n","0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n","1  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n","2  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.117647  0.168627   \n","3  0.000000  0.000000  ...  0.011765  0.000000  0.000000  0.000000  0.000000   \n","4  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n","5  0.019608  0.023529  ...  0.027451  0.031373  0.027451  0.015686  0.011765   \n","6  0.000000  0.000000  ...  0.054902  0.000000  0.000000  0.000000  0.000000   \n","7  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n","8  0.000000  0.000000  ...  0.003922  0.000000  0.000000  0.000000  0.000000   \n","9  0.000000  0.000000  ...  0.796078  0.839216  0.650980  0.000000  0.000000   \n","\n","   pixel780  pixel781  pixel782  pixel783  pixel784  \n","0  0.000000  0.000000       0.0       0.0       0.0  \n","1  0.000000  0.000000       0.0       0.0       0.0  \n","2  0.000000  0.000000       0.0       0.0       0.0  \n","3  0.003922  0.000000       0.0       0.0       0.0  \n","4  0.000000  0.000000       0.0       0.0       0.0  \n","5  0.027451  0.019608       0.0       0.0       0.0  \n","6  0.000000  0.000000       0.0       0.0       0.0  \n","7  0.000000  0.000000       0.0       0.0       0.0  \n","8  0.000000  0.000000       0.0       0.0       0.0  \n","9  0.000000  0.000000       0.0       0.0       0.0  \n","\n","[10 rows x 784 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# convert the dataset into training set and testing set\n","# for each pixel, the purpose of dividing it by 255 is to scale its value between 0 and 1 since the maximun value is 255\n","# for testset, since its label is a number, we first transform it into one-hot vector\n","\n","\n","X_train, Y_train = mnist_train.drop('label',axis=1)/255, mnist_train['label']\n","X_test, Y_test = mnist_test.drop('label',axis=1)/255, mnist_test['label']\n","Y_train, Y_test = pd.get_dummies(Y_train),pd.get_dummies(Y_test)\n","\n","# you can check the first 10 samples of the training set\n","X_train.head(10)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"z6bv_YnBKKga"},"outputs":[],"source":["# here is a function of reshaping the 1*784 vector into 28*28 matrix (the original format of a grayscale image)\n","def reshape_data(X_train, X_test):\n","  train_size, test_size = X_train.shape[0], X_test.shape[0] # get the number of train set samples and test set samples\n","  reshape_train, reshape_test = np.transpose(X_train,(1,0)), np.transpose(X_test,(1,0)) # transpose the samples to make the features of every sample listed in one row\n","  # reshape to 28*28*1, 1 is the number of channels, grey image only has 1 channel\n","  reshape_train, reshape_test = reshape_train.reshape((28,28,1,train_size)), reshape_test.reshape((28,28,1,test_size)) \n","  # reshape the sampls' dimension, make the  number of samples be the first dimension\n","  # fit in the input format of CNN: (batch_size, height, widths, channels)\n","  return np.transpose(reshape_train,(3,0,1,2)), np.transpose(reshape_test,(3,0,1,2))"]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":597,"status":"ok","timestamp":1711826890245,"user":{"displayName":"物午","userId":"12077501645191558009"},"user_tz":-480},"id":"3rj45iFQJ1u6","outputId":"a253325d-2b78-4583-ddda-9dc00376a9a7"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# here is a visualization cell that you can check the data sample from FashionMnist\n","\n","x_train_sample, x_test_sample = reshape_data(X_train.values, X_test.values) # change the label of training set and testing set to format that CNN can accept\n","y_train_sample, y_test_sample = Y_train.values, Y_test.values # extract the laels of training set and testing set\n","# the format of X_train_sample: (batch_size, height, widths, channels), batch_size = number of samples\n","# get the first sample of training set, get all lines, columns and channels\n","#  the sample data is 28*28*1 tensor, squeeze() drop out the dimension=1 axis\n","# get a 28*28 2-dimension array, which is a grey image\n","sample_image = x_train_sample[999,:, :, :].squeeze() \n","\n","# plot the figure\n","plt.imshow(sample_image, cmap='gray')\n","plt.title(\"Visualizing a sample image\")\n","plt.show()"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ True, False, False, ..., False, False, False],\n","       [False,  True, False, ..., False, False, False],\n","       [False, False,  True, ..., False, False, False],\n","       ...,\n","       [False, False, False, ..., False,  True, False],\n","       [False, False, False, ..., False,  True, False],\n","       [False,  True, False, ..., False, False, False]])"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["y_test_sample"]},{"cell_type":"markdown","metadata":{"id":"L50IwV8QLQGB"},"source":["# 1.2 Implement Fully-connected Neural Network\n","\n","In this exercise, we will implement a simple **three layer fully-connected neural network** from scratch using only **numpy** package. The NN contains one input layer with 784 nodes (dimension of the input data), one hidden layer with 128 nodes, and one output layer with 10 nodes (10 types of labels). For the first two layers, we will use `relu` as our activation function. The output layer will use `softmax` activation function. The loss for this multi-label prediction task is chosen as softmax-cross-entropy\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Cwxb7EXS85fs"},"source":["Firstly, we initialize all the parameters using the given functions. We provide an example of using this function. You can modify the function if needed (like changing the dimension etc.)."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"I5NFHWJaQuF2"},"outputs":[],"source":["def initialize_parameters(layer_dims):\n","    '''\n","    initialize all the parameters, including weights and bias\n","\n","    Inputs:\n","    - layer_dims: a list containing the number of nodes in each layer\n","\n","    Example:\n","    - if you want to build a two layer neural network\n","    - the number of nodes in each layer is 20, 10\n","    - then use the function like: para = initialize_parameters([20,10])\n","    - para['W1'] is the weight matrix from layer 1 to layer 2 with dimension (10 * 20)\n","    - you can modify the output dimension if you need\n","\n","    '''\n","\n","    parameters = {} # create an empty dictionary to store all the parameters\n","    L = len(layer_dims) # get how many layer the neural network has\n","\n","    # iterate from layer 1 to layer L-1 to initialize the weight and bias of every layer\n","    # layer 0 (input layer) doesn't have  bias and weight, just transfer parameter to next layer, so no need to initialize parameter\n","    # layer L (output layer) doesn't need initialization, because its parameters are determined by our task\n","    for l in range(1, L): \n","        # save the weight matrix from current layer to next layer to parameter dictionary\n","        # W+str(l) is the key of dictionary\n","        # convert the integer l to string, to combine with w\n","        # np.random.randn() will randomly create a (a,b) array from normal distrbution\n","        # the (a,b) is a random array containing: a: current layer node number, b:last layer node number\n","        # 0.01: make the weight to a small range, contribute to stable training process\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n","        # create an all-zero array of (current layer node number, 1) size\n","        # represent the initialized bias are all zero\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n","\n","    return parameters"]},{"cell_type":"markdown","metadata":{"id":"LiwOro3x1HUh"},"source":["# 1.2.1 Activation functions and Loss functions\n","\n","In this section, you will implement different functions, including `relu`, `softmax` and `cross entropy loss`. The formulas are listed below for reference.\n","\n","---\n","*   $\\text{ReLU}(z) = \\max(0, z)$ \\\\\n","*   $\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$\n","*   $L = -\\sum_{c=1}^{K} y_c \\log(\\hat{y}_c)$, where $y_c$ is the ground truth labels, $\\hat{y}_c$ is the prediced labels\n","\n","\n","\n","---\n","\n","**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below three functions.\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"YsKrO2HXQxEu"},"outputs":[],"source":["def relu(Z):\n","  '''\n","  the ReLU activation function\n","\n","  Inputs:\n","  - Z: the computed results before activation (e.g. WX+b)\n","\n","  Outputs:\n","  - A: the result after activation\n","\n","  '''\n","  # compare every element of z with 0, if >0, output the element itself, otherwise output 0\n","  A = np.maximum(0, Z)\n","\n","  return A\n","\n","\n","def softmax(Z):\n","\n","  A = np.exp(Z) / sum(np.exp(Z))\n","\n","  return A\n","\n","\n","def compute_loss(A, Y):\n","\n","  '''\n","  the softmax activation function\n","\n","  Inputs:\n","  - A: the output result after softmax, the dimension is [10, batch_size]\n","  - Each row of A is a probability vector, A[0,0] represents the probability of the first sample belonging to label 0\n","  - Y: the groundtruth label, the dimension is [10, batch_size]\n","\n","  Outputs:\n","  - L: the computed loss\n","\n","  '''\n","  # shape[1] is Y's second dimension: batch size\n","  # dividing by batch size make the total loss become average loss between every sample\n","  L = - np.sum(Y * np.log(A)) / Y.shape[1]\n","\n","  return L\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4fisu3rKC5n7"},"source":["# 1.2.2 Forward Propagation\n","\n","In this section, you need to implement the `forward_propagation` function. Each time you propagate from one layer to the next layer, two steps are needed:\n","\n","\n","1.   Multiply the values in the current nodes with the weight matrix and add the bias terms\n","2.   Use the activation function to activate the results\n","\n","In our setting, we have three layers, so we need to propogate our training data twice to get the final prediction results.\n","\n","**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function.\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"AKjJoEbDGeaJ"},"outputs":[],"source":["def forward_propagation(X, parameters):\n","\n","    '''\n","    Inputs:\n","    - X: the training data, with dimension [784,batch_size]\n","    - parameters: a dictionary that contains all the parameters define\n","\n","    Outputs:\n","    - A2: the result of the last layer\n","    - cache: a tuple that stores the propagation results for later backpropagation process\n","\n","    '''\n","    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n","\n","    # Step 1: Compute the activations of the first layer\n","    Z1 = W1.dot(X) + b1\n","    A1 = relu(Z1)  # Apply ReLU activation function\n","\n","    # Step 2: Compute the activations of the second layer\n","    Z2 = W2.dot(A1) + b2\n","    A2 = softmax(Z2)  # Apply softmax activation function\n","\n","    # Store intermediate results for backpropagation\n","    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n","    return A2, cache"]},{"cell_type":"markdown","metadata":{"id":"BFp78qaqGlRJ"},"source":["# 1.2.3 Backpropagation\n","\n","In this section, we will implement the backpropagation process by completing the `relu_derivative` and `backward_propagation` functions.\n","\n","When doing the backpropagation, we mainly use chain rule to compute the gradient. Below is an example that helps you understand this process:\n","\n","Suppose we have two layers, we have the input data $X$, weight matrix $W$, forward result (without activation) $Z$, activation function $σ$, forward result (after activation) $A$, ground truth label $Y$ and Loss $L(Y,A)$. And we have the below relationships:\n","\n","$Z = WX$\n","\n","$A = σ(Z)$\n","\n","If you wanna compute the gradient of the weight matrix, the formula is:\n","\n","$\\frac{dL}{dW} = \\frac{dL}{dA}\\frac{dA}{dZ}\\frac{dZ}{dW} = \\frac{dL}{dA}\\frac{dA}{dZ} × X$, and $\\frac{dL}{dA}$, $\\frac{dA}{dZ}$ can be directly computed according to your loss function and activation function.\n","\n","When implement this function, recall that we have stored the forward result and parameters in the cache. Please fully use these infomation to do your backpropagation.\n","\n","**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"gzTlHq1T0N_c"},"outputs":[],"source":["def relu_derivative(Z):\n","    '''\n","    Inputs:\n","    - Z: the value before activation\n","\n","    Outputs：\n","    - dZ： the derivative of relu function\n","    '''\n","    # copy z to dz\n","    dZ = Z > 0\n","\n","    return dZ\n","\n","def backward_propagation(parameters, cache, X, Y):\n","    '''\n","    Implement the backward propagation process for our 3 layer NN\n","    Here are several steps you need to do:\n","    1. dZ2: Compute the gradient of Cross Entropy Loss and Softmax Function\n","    2. dW2: Compute the gradient of the weight matrix from hidden layer to output layer\n","    3. db2: Compute the gradient of the bias term from hidden layer to output layer\n","    4. dA2: Compute the gradient of the ReLU activation\n","    ...\n","\n","    Inputs:\n","    - paramters: a dictionary containing all the parameters\n","    - cache: a tuple that store all the forward propagation results\n","    - X: training data, dimension of [784, batch_size]\n","    - Y: ground truth label, dimension of [10, batch_size]\n","\n","    Outputs:\n","    - grads: a dictionary containing all the computed gradients\n","    '''\n","    Z1, A1, W1, b1, Z2, A2, W2, b2 = cache # load the forward results and parameters from forward propagation\n","\n","    dZ2, dW2, db2, dA2, dZ1, dW1, db1 = None, None, None, None, None, None, None\n","\n","    m = Y.size\n","    dZ2 = A2 - Y  # Compute the gradient of Cross Entropy Loss and Softmax Function\n","    dW2 = 1 / m * dZ2.dot(A1.T)  # Compute the gradient of the weight matrix from hidden layer to output layer\n","    db2 = 1 / m * np.sum(dZ2) # Compute the gradient of the bias term from hidden layer to output layer\n","\n","    dZ1 = W2.T.dot(dZ2) * relu_derivative(Z1)  # Compute the gradient of the ReLU activation\n","    dW1 = 1 / m * dZ1.dot(X.T)\n","    db1 = 1 / m * np.sum(dZ1)\n","\n","\n","    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n","\n","    return grads"]},{"cell_type":"markdown","metadata":{"id":"I1o_--PwYxvF"},"source":["# 1.2.4 Update Parameters\n","\n","Now, we have finished the backward propagation and get all the computed gradients. We can use the computed gradients to update all the parameters using gradient descent methods."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"XcSW4DwsOrSa"},"outputs":[],"source":["def update_parameters(parameters, grads, learning_rate):\n","\n","    L = len(parameters) // 2  # number of layers\n","\n","    for l in range(L):\n","        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n","\n","    return parameters"]},{"cell_type":"markdown","metadata":{"id":"DSDr4s8MaYMa"},"source":["# 1.2.5 Training our NN\n","\n","In the previous section, we successfully implement the `forward` and `backward` part of NN. Now we can combine all the elements together and train our neural network.\n","\n","In real training process, we don't feed all the data into the neural network at one epoch. Instead, we divide the dataset into small batches and update the parameters using just a small batch of data each time. The introduction of small bacth can reduce the memory load and increase the randomness, which can facilitate the training process and increase the generalization capability of our model. Below we provide two functions that can devide the training set and testing set into small batches."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"p2GOpSnI00-k"},"outputs":[],"source":["def create_train_batches(X, Y, batch_size):\n","    '''\n","    Inputs:\n","    - X: training data, dimension [784, 60000]\n","    - Y: ground truth label. dimension [10, 60000]\n","    - batch_size: the size of each batch\n","\n","    Outputs:\n","    - mini_batches: a list containing all the data divided into different batches\n","    '''\n","    m = X.shape[1]  # number of data; (60000)\n","    mini_batches = [] # create an empty list to store devided small batches data\n","\n","    # shuffle all the data\n","    # mess up the order of the batches data to increase the randomness of data\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = X[:, permutation]\n","    shuffled_Y = Y[:, permutation]\n","\n","    # divide the data into batch\n","    num_complete_minibatches = m // batch_size\n","    for k in range(0, num_complete_minibatches):\n","        mini_batch_X = shuffled_X[:, k*batch_size : (k+1)*batch_size]\n","        mini_batch_Y = shuffled_Y[:, k*batch_size : (k+1)*batch_size]\n","        mini_batches.append((mini_batch_X, mini_batch_Y))\n","\n","    # deal with the last batch (may not equal to the batch_size)\n","    # the datasize may not be completely divided by batch_size\n","    # add the remaining small batch to mini batches\n","    if m % batch_size != 0:\n","        mini_batch_X = shuffled_X[:, num_complete_minibatches*batch_size :]\n","        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*batch_size :]\n","        mini_batches.append((mini_batch_X, mini_batch_Y))\n","\n","    return mini_batches\n","\n","# similar to the first function, but for testing data there is no need to deal with the label data\n","def create_test_batches(X, batch_size):\n","    '''\n","    Inputs:\n","    - X: testing data, dimension [784, 10000]\n","    - batch_size: the size of each batch\n","\n","    Outputs:\n","    - mini_batches: a list containing all the data divided into different batches\n","    '''\n","    mini_batches = []\n","    m = X.shape[1]  # number of data, (10000)\n","    n_batches = m // batch_size\n","\n","    for i in range(n_batches):\n","        X_mini = X[:, i*batch_size:(i+1)*batch_size]\n","        mini_batches.append(X_mini)\n","\n","    if m % batch_size != 0:\n","        X_mini = X[:, n_batches*batch_size:]\n","        mini_batches.append(X_mini)\n","\n","    return mini_batches"]},{"cell_type":"markdown","metadata":{"id":"oK5ofsELdf0P"},"source":["Before training our NN, we need to define a `model_predict` function to predict the labels of the data samples, which can be further used to compute the prediction accuracy.\n","\n","**Hint 1**: Since our data are seperated into small batches, you need to predict the labels of data in each batch and then concatenate them together to the final predictions\n","\n","**Hint 2**: Recall that output result of our NN is a one-hot probability vector for each data sample. You should convert them into a single label\n","\n","**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"XcQfu2Gcck2H"},"outputs":[],"source":["def model_predict(X_test, parameters, batch_size):\n","    mini_batches = create_test_batches(X_test, batch_size)\n","    all_predictions = [] # a list containing the prediction result of each small batch\n","\n","    for X_mini in mini_batches:\n","      # Forward propagation to get predictions\n","      # forward propragation function will output distribution of output layer and a useless cache value_\n","      A2, _ = forward_propagation(X_mini, parameters)\n","      # Convert one-hot probability vectors to single labels\n","      # along axis=0(dimension of batch) find the label whose probability is maximazed\n","      predictions = np.argmax(A2, 0)\n","      all_predictions.append(predictions)\n","\n","    all_predictions = np.concatenate(all_predictions, axis=0)\n","    return all_predictions"]},{"cell_type":"markdown","metadata":{"id":"Vh8WpO-Ug84O"},"source":["Finally, we can train our NN. You should following the below training steps to train your NN for each small batch in each iteration :\n","\n","\n","\n","*   Do the **Forward Propagation** to get the forward results\n","*   Compute the **Loss** based on the forward results\n","*   Do the **Backward Propagation** to compute all the gradients\n","*   Update all the parameters\n","\n","After iteration all the small batches, you should calculate the accuracy for both training data and testing data in this iteration.\n","\n","**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"WJG44uxE0ugA"},"outputs":[],"source":["# layers_dims: dimension of neural network layer\n","def train(X_train, Y_train, X_test, Y_test, layers_dims, learning_rate, num_iterations, batch_size):\n","    # initialize the parameters\n","    parameters = initialize_parameters(layers_dims)\n","\n","    # training iterations\n","    for i in range(0, num_iterations):\n","\n","        # create the small batches\n","        mini_batches = create_train_batches(X_train, Y_train, batch_size)\n","\n","        for mini_batch in mini_batches:\n","            (mini_batch_X, mini_batch_Y) = mini_batch\n","\n","            # Forward propagation to get forward results\n","            A2, cache = forward_propagation(mini_batch_X, parameters)\n","            \n","            # Compute loss\n","            cost = compute_loss(A2, mini_batch_Y)\n","\n","            # Backward propagation to compute gradients\n","            grads = backward_propagation(parameters, cache, mini_batch_X, mini_batch_Y)\n","\n","            # Update parameters\n","            parameters = update_parameters(parameters, grads, learning_rate)\n","\n","        # accuracy of training\n","        all_predictions = model_predict(X_train, parameters, batch_size)\n","        train_labels = np.argmax(Y_train, axis=0)\n","        train_acc = np.mean(all_predictions == train_labels) # the portion of accurate(True)\n","\n","        # predict\n","        all_predictions = model_predict(X_test, parameters, batch_size)\n","        test_labels = np.argmax(Y_test, axis=0)\n","        test_acc = np.mean(all_predictions == test_labels)\n","\n","        # print cost and acc\n","        print (\"Cost after iteration %i: %f | Training Accuracy: %f | Test Accuracy: %f\" %(i+1, cost, train_acc, test_acc))\n","\n","    return parameters"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"-FNFPDu1eCk4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cost after iteration 1: 2.229989 | Training Accuracy: 0.373967 | Test Accuracy: 0.376000\n","Cost after iteration 2: 1.970875 | Training Accuracy: 0.413133 | Test Accuracy: 0.416200\n","Cost after iteration 3: 1.505368 | Training Accuracy: 0.507800 | Test Accuracy: 0.506300\n","Cost after iteration 4: 1.359002 | Training Accuracy: 0.592800 | Test Accuracy: 0.589300\n","Cost after iteration 5: 1.109903 | Training Accuracy: 0.638783 | Test Accuracy: 0.634000\n","Cost after iteration 6: 1.108957 | Training Accuracy: 0.684733 | Test Accuracy: 0.682100\n","Cost after iteration 7: 0.784727 | Training Accuracy: 0.710700 | Test Accuracy: 0.709100\n","Cost after iteration 8: 0.819733 | Training Accuracy: 0.723533 | Test Accuracy: 0.721600\n","Cost after iteration 9: 0.900604 | Training Accuracy: 0.730700 | Test Accuracy: 0.727000\n","Cost after iteration 10: 0.712111 | Training Accuracy: 0.731383 | Test Accuracy: 0.727000\n","Cost after iteration 11: 0.799665 | Training Accuracy: 0.738150 | Test Accuracy: 0.734900\n","Cost after iteration 12: 0.594715 | Training Accuracy: 0.746183 | Test Accuracy: 0.746000\n","Cost after iteration 13: 0.601428 | Training Accuracy: 0.749700 | Test Accuracy: 0.747000\n","Cost after iteration 14: 0.900999 | Training Accuracy: 0.754383 | Test Accuracy: 0.752700\n","Cost after iteration 15: 0.530056 | Training Accuracy: 0.757967 | Test Accuracy: 0.757300\n","Cost after iteration 16: 0.557308 | Training Accuracy: 0.759933 | Test Accuracy: 0.761300\n","Cost after iteration 17: 0.596692 | Training Accuracy: 0.765817 | Test Accuracy: 0.767700\n","Cost after iteration 18: 0.588767 | Training Accuracy: 0.769767 | Test Accuracy: 0.771900\n","Cost after iteration 19: 0.678708 | Training Accuracy: 0.772050 | Test Accuracy: 0.772800\n","Cost after iteration 20: 0.585255 | Training Accuracy: 0.776883 | Test Accuracy: 0.778100\n"]}],"source":["# run this script to train you NN\n","\n","x_train = X_train.values.T # reshape it from [60000,784] into [784,60000], because every column represents a batch in NN\n","y_train = Y_train.values.T # make sure the shape of label data is consistent with input data shape\n","x_test = X_test.values.T\n","y_test = Y_test.values.T\n","\n","\n","para = train(x_train , y_train, x_test, y_test, layers_dims = [784,128,10], learning_rate = 0.05, num_iterations = 20, batch_size = 256)"]},{"cell_type":"markdown","metadata":{"id":"Hz11VjdxS-As"},"source":["# Convolutional Neural Network"]},{"cell_type":"markdown","metadata":{"id":"8l-Hp7Srks2-"},"source":["In this exercise, we are going to implement a convoutional neural network from scratch using numpy. The network structure is shown as below:\n","\n","*   `conv_layer`: 32 2*2 convolutional kernels; stride = 1;\n","*   `max_pooling`: 2*2 pooling window; stride = 2;\n","*   `fully-connected layers`: 3 two connected layers, each with n_flatten, 64, 10 nodes. `n_flatten` represents the dimension of the flattened pooling result\n","\n","You will be asked to implement the `forward` part, including `conv_forward`, `pool_forward` and `linear_forward`. The backward part and training process are given. If needed, you can also modify those given functions to align with your implementation.\n","\n","**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function."]},{"cell_type":"markdown","metadata":{"id":"PcF_uw9goL7Y"},"source":["# 2.1 Forward Propagation"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"HxwqnmY2S9s5"},"outputs":[],"source":["def conv_forward(A_prev, W, b, stride):\n","    \"\"\"\n","    Implement the forward of conv layers (with bias)\n","\n","    Inputs:\n","    - A_prev: The activation output from the previous layer, with dimension of (m, n_H_prev, n_W_prev, n_C_prev)\n","    - (1) m is the batch size (2) n_H_prev is the height of previous output\n","    - (3) n_W_prev is the width of the previous output (4) n_C_prev is the number of channels from the previous output\n","\n","    - W: The weight of the conv kernel, with dimension of: (f, f, n_C_prev, n_C)\n","    - (1) f is the size of the kernel (2) n_C is the number of the conv kernels\n","\n","    - b: The bias term with dimension of (1, 1, 1, n_C)\n","    - stride: A integer represents how far the conv kernel moves each time\n","\n","\n","    Outputs:\n","    - Z: The conv result, with dimension of (m, n_H, n_W, n_C)\n","    - A_prev, W, b, stride\n","\n","    \"\"\"\n","\n","    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n","    (f, f, n_C) = W.shape\n","\n","    # TODO: Compute the height and weight of the output after convolution  #\n","    # Replace None with your formula                       #\n","    n_H = int((n_H_prev - f) / stride) + 1\n","    n_W = int((n_W_prev - f) / stride) + 1\n","\n","\n","    # initialize the output\n","    Z = np.zeros((m, n_H, n_W, n_C))\n","\n","    # TODO: Implement the convolutional process                  #\n","    # Hint: You can use a for loop to iterate all samples, H, W, C    #\n","\n","    for i in range(m):                     # loop over the batch of training examples\n","        for h in range(n_H):               # loop over vertical axis of the output volume\n","            for w in range(n_W):           # loop over horizontal axis of the output volume\n","                for c in range(n_C):       # loop over channels (= #filters) of the output volume\n","\n","                    # Find the corners of the current \"slice\"\n","                    vert_start = h * stride\n","                    vert_end = vert_start + f\n","                    horiz_start = w * stride\n","                    horiz_end = horiz_start + f\n","\n","                    # Use the corners to define the (3D) slice of a_prev_pad\n","                    a_slice_prev = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, :]\n","\n","                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron\n","                    Z[i, h, w, c] = np.sum(a_slice_prev * W[ :, :, c]) + b[:, :, :, c]\n","    \n","\n","\n","    assert(Z.shape == (m, n_H, n_W, n_C))\n","\n","    return Z, A_prev, W, b, stride"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"9STy2sD1TECp"},"outputs":[],"source":["def pool_forward(A_prev, f, stride):\n","    \"\"\"\n","    Inputs:\n","    - A_prev: The activation output from the previous layer, with dimension of (m, n_H_prev, n_W_prev, n_C_prev)\n","    - (1) m is the batch size (2) n_H_prev is the height of previous output\n","    - (3) n_W_prev is the width of the previous output (4) n_C_prev is the number of channels from the previous output\n","\n","    - f: Integer, the height and width of the pooling windows\n","    - stride: Integer, indicate how far the pooling move each time\n","\n","    Outputs:\n","    - A: The output of the pooling layers, with dimension of (m, n_H, n_W, n_C)\n","    - A_prev, f, stride, n_H, n_W, n_C\n","\n","    \"\"\"\n","    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n","\n","    # TODO: Compute the height and weight of the output after convolution  #\n","    # Replace None with your formula                       #\n","    n_H = int(1 + (n_H_prev - f) / stride)\n","    n_W = int(1 + (n_W_prev - f) / stride)\n","    n_C = n_C_prev\n","\n","    # initialize the output\n","    A = np.zeros((m, n_H, n_W, n_C))\n","\n","    # TODO: Implement the Pooling process                     #\n","    # Hint: You can use a for loop to iterate all samples, H, W, C    #\n","    \n","    for i in range(m):                     # loop over the batch of training examples\n","        for h in range(n_H):               # loop over vertical axis of the output volume\n","            for w in range(n_W):           # loop over horizontal axis of the output volume\n","                for c in range(n_C):       # loop over channels (= #filters) of the output volume\n","\n","                    # Find the corners of the current \"slice\"\n","                    vert_start = h * stride\n","                    vert_end = vert_start + f\n","                    horiz_start = w * stride\n","                    horiz_end = horiz_start + f\n","\n","                    # Use the corners to define the current slice on the ith training example of A_prev, channel c\n","                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n","\n","                    # Compute the pooling operation on the slice\n","                    A[i, h, w, c] = np.max(a_prev_slice)\n","\n","    assert(A.shape == (m, n_H, n_W, n_C))\n","\n","\n","    return A, A_prev, f, stride, n_H, n_W, n_C"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"9iKLb3lqTG6j"},"outputs":[],"source":["def fully_connected_forward(A_prev, W, b):\n","    \"\"\"\n","    Inputs:\n","    - A_prev: activation output from the last layer，with a dimension of (batch_size, # of nodes in last layer)\n","    - W: Weigh Matrix, with a dimension of (# of nodes in last layer, # of nodes in this layer)\n","    - b: Bias Term, with a dimension of (# of nodes in this layer, 1)\n","\n","    Outputs:\n","    - Z: The output result without activation\n","    \"\"\"\n","\n","    # Z = W.dot(A_prev) + b\n","\n","    Z = A_prev.dot(W)+b\n","    \n","    # TODO: Implement the fc forward process                   #\n","    \n","    return Z, W, b\n"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"-XqkzIXnTKK5"},"outputs":[],"source":["def cnn_forward(X, parameters):\n","\n","    cache = []\n","\n","    # First Conv\n","    Z1, A0, W1, b1, s1 = conv_forward(X, parameters['W1'], parameters['b1'], stride=1)\n","    # print('Z1 =',Z1.shape,'A0 =',A0.shape)\n","    A1 = relu(Z1)\n","    \n","    # Max Pooling\n","    P1, A1, f, s2, H2, W2, C2 = pool_forward(A1, f=2, stride=2)\n","    # print('P1 =',P1.shape,'A1 =',A1.shape)\n","    # Flatten\n","    P1_flattened = P1.reshape(P1.shape[0], -1)\n","    # print('P1_flattened =',P1_flattened.shape, 'P1 =',P1.shape)\n","    \n","    # FC Layers 1\n","    # print('P1_flattened, W2 shape, b2 shape',P1_flattened.shape, parameters['W2'].shape, parameters['b2'].shape)\n","    Z2, W3, b3 = fully_connected_forward(P1_flattened, parameters['W2'], parameters['b2'])\n","    A2 = relu(Z2)\n","\n","    # FC Layers 2\n","    Z3, W4, b4 = fully_connected_forward(A2, parameters['W3'], parameters['b3'])\n","    A3 = softmax(Z3)\n","\n","    # conv cache --> pooling cache --> fc 1 cache --> fc 2 cache\n","    cache.append((Z1, A0, W1, b1, s1))\n","    cache.append((P1, A1, f, s2, H2, W2, C2))\n","    cache.append((Z2, P1_flattened, W3, b3))\n","    cache.append((Z3, A2, W4, b4))\n","\n","    return A3, cache\n"]},{"cell_type":"markdown","metadata":{"id":"moG49F1cuCW1"},"source":["# Backpropagation"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"v7wTg6hWVNnz"},"outputs":[],"source":["def relu_backward(dA, cache):\n","    Z, A_prev, W, b = cache\n","    dZ = np.array(dA, copy=True)\n","    dZ[Z <= 0] = 0\n","    return dZ\n","\n","def conv_backward(dZ, cache):\n","    (Z, A_prev, W, b, stride) = cache\n","    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n","    # print(W.shape)\n","    (f, f, n_C) = W.shape\n","    (m, n_H, n_W, n_C) = dZ.shape\n","\n","    # initialize the gradients\n","    dA_prev = np.zeros_like(A_prev)\n","    dW = np.zeros_like(W)\n","    db = np.zeros_like(b)\n","\n","    for i in range(m):\n","        a_prev = A_prev[i]\n","        for h in range(n_H):\n","            for w in range(n_W):\n","                for c in range(n_C):\n","                    # localize the current slice\n","                    x_start = h * stride\n","                    x_end = x_start + f\n","                    y_start = w * stride\n","                    y_end = y_start + f\n","                    a_slice = a_prev[x_start:x_end, y_start:y_end, :]\n","\n","                    # compute the gradients\n","                    dA_prev[i, x_start:x_end, y_start:y_end, :] += (W[:, :, c] * dZ[i, h, w, c]).reshape(2,2,1)\n","                    dW[:, :, c] += (a_slice * dZ[i, h, w, c]).reshape(2,2)\n","                    db[:, :, :, c] += dZ[i, h, w, c]\n","\n","    # compute the bias gradients\n","    db = np.sum(dZ, axis=(0, 1, 2), keepdims=True) / m\n","\n","    return dA_prev, dW, db\n","\n","\n","def create_mask_from_window(x):\n","    mask = x == np.max(x)\n","    return mask\n","\n","def pool_backward(dA, cache):\n","    (P, A_prev, f, stride, n_H, n_W, n_C) = cache\n","    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n","    bs = dA.shape[0]\n","    dA = dA.reshape(bs ,n_H ,n_W ,n_C) # unflatten\n","\n","    dA_prev = np.zeros_like(A_prev)\n","\n","    for i in range(m):\n","        a_prev = A_prev[i]\n","        for h in range(n_H):\n","            for w in range(n_W):\n","                for c in range(n_C):\n","                    x_start = h * stride\n","                    x_end = x_start + f\n","                    y_start = w * stride\n","                    y_end = y_start + f\n","                    a_prev_slice = a_prev[x_start:x_end, y_start:y_end, c]\n","                    mask = create_mask_from_window(a_prev_slice)\n","                    dA_prev[i, x_start:x_end, y_start:y_end, c] += mask * dA[i, h, w, c]\n","    return dA_prev\n","\n","def distribute_value(dz, shape):\n","    (n_H, n_W) = shape\n","    average = dz / (n_H * n_W)\n","    a = np.ones(shape) * average\n","    return a\n","\n","def linear_backward(dZ, cache):\n","    Z, A_prev, W, b= cache\n","    m = A_prev.shape[0]\n","    dW = 1./m * np.dot(A_prev.T, dZ)\n","    db = 1./m * np.sum(dZ, axis=0, keepdims=True)\n","    dA_prev = np.dot(dZ,W.T)\n","    return dA_prev, dW, db\n","\n","def cnn_backward(AL, Y, caches):\n","\n","    gradients = {}\n","\n","    # compute the gradient of the output layer dAL which uses the softmax activation\n","    dAL = AL - Y\n","\n","\n","    # backward of the second fc\n","    current_cache = caches[-1]\n","    gradients[\"dA2\"], gradients[\"dW3\"], gradients[\"db3\"] = linear_backward(dAL, current_cache)\n","\n","    # backward of the relu function + the first lc\n","    current_cache = caches[-2]\n","    dZ = relu_backward(gradients[\"dA2\"], current_cache)\n","    gradients[\"dA1\"], gradients[\"dW2\"], gradients[\"db2\"] = linear_backward(dZ, current_cache)\n","    # print(gradients[\"dW2\"].shape)\n","\n","    # backward of max pooling --> relu --> backward of conv\n","    current_cache = caches[-3]\n","    dA0 = pool_backward(gradients[\"dA1\"], current_cache)\n","\n","    Z1, A0, W1, b1, s1 = caches[0]\n","    current_cache = (Z1, A0, W1, b1)\n","    dZ = relu_backward(dA0, current_cache)\n","\n","    current_cache = caches[0]\n","    gradients[\"dA0\"], gradients[\"dW1\"], gradients[\"db1\"] = conv_backward(dZ, current_cache)\n","\n","    return gradients"]},{"cell_type":"markdown","metadata":{"id":"fLr7Ho5uwAPO"},"source":["# Update Parameters"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"Pmak-ncXafRy"},"outputs":[],"source":["def update_parameters(parameters, grads, learning_rate):\n","    L = len(parameters) // 2  # number of layers\n","\n","    for l in range(1, L+1):\n","        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n","        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n","\n","    return parameters\n"]},{"cell_type":"markdown","metadata":{"id":"5HohRhwXwO2P"},"source":["# Creating Batch Data"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"14S9h_8e6GGF"},"outputs":[],"source":["def create_minibatches(X, Y, minibatch_size):\n","\n","    m = X.shape[0]\n","    minibatches = []\n","\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = X[permutation, :, :, :]\n","    shuffled_Y = Y[permutation, :]\n","\n","    num_complete_minibatches = m // minibatch_size\n","\n","    for k in range(0, num_complete_minibatches):\n","        minibatch_X = shuffled_X[k * minibatch_size:(k + 1) * minibatch_size, :, :, :]\n","        minibatch_Y = shuffled_Y[k * minibatch_size:(k + 1) * minibatch_size, :]\n","        minibatch = (minibatch_X, minibatch_Y)\n","        minibatches.append(minibatch)\n","\n","    if m % minibatch_size != 0:\n","        minibatch_X = shuffled_X[num_complete_minibatches * minibatch_size:, :, :, :]\n","        minibatch_Y = shuffled_Y[num_complete_minibatches * minibatch_size:, :]\n","        minibatch = (minibatch_X, minibatch_Y)\n","        minibatches.append(minibatch)\n","\n","    return minibatches"]},{"cell_type":"markdown","metadata":{"id":"EGvPibHsxQaK"},"source":["# Training Your CNN\n","\n","**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part to complete the training process"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"2QlAQtdTWEqJ"},"outputs":[],"source":["# The first step is initializing the parameters\n","\n","parameters = {}\n","\n","# Initilalize the parameters of conv kernels\n","parameters['W' + str(1)] = np.random.randn(2, 2, 16) * np.sqrt(2/68)\n","parameters['b' + str(1)] = np.zeros((1, 1, 1, 16))\n","\n","\n","\n","# TODO: Based on the network structure, compute the dimension of the flatten pooling result  #\n","\n","# Compute the dimension of the flatten pooling result\n","n_L_prev = (28-2)//1 + 1  # Formula for convolutional layer output size\n","n_L_prev = (n_L_prev - 2) // 2 + 1  # Formula for max pooling layer output size\n","n_L_prev *= n_L_prev * 16  # 16 is the number of filters in the convolutional layer\n","\n","# Initialize the paramters of the FC Layers\n","parameters['W' + str(2)] = np.random.randn(n_L_prev, 64) * np.sqrt(2 / (n_L_prev + 64))\n","parameters['b' + str(2)] = np.zeros((1,64))\n","parameters['W' + str(3)] = np.random.randn(64, 10) * np.sqrt(2 / (64+10))\n","parameters['b' + str(3)] = np.zeros((1,10))"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["(2704, 64)"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["parameters['W2'].shape"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"k5eQrgdSXQFu"},"outputs":[],"source":["def predict_accuracy(X, Y, parameters):\n","    probas, caches = cnn_forward(X, parameters)\n","    predicted_labels = np.argmax(probas, axis=1)\n","    true_labels = np.argmax(Y, axis=1)\n","    accuracy = np.mean(predicted_labels == true_labels)\n","    return accuracy\n","\n","def compute_cost(AL, Y):\n","    m = Y.shape[0]\n","    cost = -1/m * np.sum(Y * np.log(AL))\n","    return cost\n","\n","def train_cnn(X_train, Y_train, X_test, Y_test, parameters, learning_rate=0.001, num_epochs=100, batch_size=64):\n","\n","    costs = []  # list that stores cost\n","\n","    for i in range(num_epochs):\n","        mini_batches = create_minibatches(X_train, Y_train, batch_size)\n","        # print('X_train shape =',X_train.shape)\n","        cost_total = 0\n","\n","        for minibatch in mini_batches:\n","            (minibatch_X, minibatch_Y) = minibatch\n","            # print('minibatch_X =',minibatch_X.shape)\n","            \n","            # TODO: finish the training process              #\n","            \n","            # Forward propagation\n","            AL, caches = cnn_forward(minibatch_X, parameters)\n","            # print(minibatch_X,parameters)\n","\n","            # Compute cost\n","            cost_total += compute_cost(AL, minibatch_Y)\n","\n","            # Backward propagation\n","            gradients = cnn_backward(AL, minibatch_Y, caches)\n","\n","            # Update parameters\n","            parameters = update_parameters(parameters, gradients, learning_rate)\n","\n","        cost_avg = cost_total / len(mini_batches)\n","        costs.append(cost_avg)\n","\n","        # Print the Accuracy\n","\n","        test_accuracy = predict_accuracy(X_test, Y_test, parameters)\n","        print(f\"Epoch {i+1}/{num_epochs}, Cost: {cost_avg}, Test Accuracy: {test_accuracy}\")\n","\n","    return parameters, costs"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"tOZTMigf2ipH"},"outputs":[],"source":["# get all the training and testing data\n","x_train, x_test = reshape_data(X_train.values, X_test.values)\n","y_train, y_test = Y_train.values, Y_test.values"]},{"cell_type":"markdown","metadata":{"id":"xFqqoGCbyZrW"},"source":["Run this cell to train you CNN. Since we use Numpy to implement CNN from scratch, the training speed may not be fast. Therefore, when testing your data, you can use a very small dataset to verify all your codes run well. For example, you can use 20 training samples and 1 test samples. After that, you can use a larger subset of the dataset to present your results.\n","\n","Since this CNN is trained on a subset of the whole dataset, you get the marks if the result is reasonable.\n","\n","**Reference**: Using 1000 training samples and 100 testing samples for 5 epochs will take like 40mins in TA's vanilla implementation; Using 100 training samples and 10 testing samples for 5 epochs will take 3-5 mins. It's normal that the model becomes overfitting if you only use small subset of dataset."]},{"cell_type":"code","execution_count":77,"metadata":{"id":"eoPRyoCtZ_Up"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Cost: 4.803460267285803, Test Accuracy: 0.28\n","Epoch 2/10, Cost: 4.792584668078145, Test Accuracy: 0.28\n","Epoch 3/10, Cost: 4.774080251922021, Test Accuracy: 0.28\n","Epoch 4/10, Cost: 4.740941017764231, Test Accuracy: 0.24\n","Epoch 5/10, Cost: 4.698267946809978, Test Accuracy: 0.3\n","Epoch 6/10, Cost: 4.699435450900795, Test Accuracy: 0.38\n","Epoch 7/10, Cost: 5.135735345265621, Test Accuracy: 0.44\n","Epoch 8/10, Cost: 6.754432667991933, Test Accuracy: 0.28\n","Epoch 9/10, Cost: 12.280867615011381, Test Accuracy: 0.26\n","Epoch 10/10, Cost: 31.714942501612803, Test Accuracy: 0.14\n"]}],"source":["# Replace all the None to the subset of the dataset\n","x_train_subset = x_train[:1000]\n","x_test_subset = x_test[:50]\n","y_train_subset = y_train[:1000]\n","y_test_subset = y_test[:50]\n","para, cost = train_cnn(x_train_subset, y_train_subset, x_test_subset, y_test_subset, parameters, learning_rate=0.001, num_epochs=10, batch_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOjy408ew0WEW+5EMQKv2NY","mount_file_id":"1AdGyiVYosU2uMBQ9ahx5B7ZOmH8zEQg_","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
